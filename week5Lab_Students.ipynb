{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Done by Eris Leksi (9067882)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Tidying**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Part of the data engineering process is data cleaning and tidying. What is done in those two processes, is trying to make the data more readable, and complete. This makes much easier to analyze, visualize, and train the data.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Tidying**\n",
    "\n",
    "Making the data more organized, and readable is the result of applying data tidying. \n",
    "\n",
    "In this section two main pandas functions are used in data tidying those are `melt` and `pivot_table`."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by taking a look at the below dataframe, which represents the income ranges based on religion. This is part of the PEW research, which is famous in the US for conducting pollings and surveys on citizens.\n",
    "\n",
    "When the following are satisfied:\n",
    "\n",
    "\n",
    "1. Each variable forms a column\n",
    "2. Each observation forms a row\n",
    "3. Each type of observational unit forms a table\n",
    "\n",
    "We can then say that our dataset is *tidy*.\n",
    "\n",
    "First we need to import pandas to read csv datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **PEW Research Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Start by Importing the dataset into a pandas dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pew = pd.read_csv('./Data/pew-raw.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the dataset using the `loc`, `iloc`, `head`, or `tail` approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  $50-75k\n",
      "0                  Agnostic      137\n",
      "1                   Atheist       70\n",
      "2                  Buddhist       58\n",
      "3                  Catholic     1116\n",
      "4         Dont know/refused       35\n",
      "5         Evangelical Prot      1486\n",
      "6                    Hindu        34\n",
      "7  Historically Black Prot       223\n",
      "8         Jehovahs Witness        30\n",
      "9                   Jewish        95\n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "                   religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  \\\n",
      "0                  Agnostic      27        34       60       81        76   \n",
      "1                   Atheist      12        27       37       52        35   \n",
      "2                  Buddhist      27        21       30       34        33   \n",
      "3                  Catholic     418       617      732      670       638   \n",
      "4         Dont know/refused      15        14       15       11        10   \n",
      "5         Evangelical Prot      575       869     1064      982       881   \n",
      "6                    Hindu        1         9        7        9        11   \n",
      "7  Historically Black Prot      228       244      236      238       197   \n",
      "8         Jehovahs Witness       20        27       24       24        21   \n",
      "9                   Jewish       19        19       25       25        30   \n",
      "\n",
      "   $50-75k  \n",
      "0      137  \n",
      "1       70  \n",
      "2       58  \n",
      "3     1116  \n",
      "4       35  \n",
      "5     1486  \n",
      "6       34  \n",
      "7      223  \n",
      "8       30  \n",
      "9       95  \n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "            religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  $50-75k\n",
      "0           Agnostic      27        34       60       81        76      137\n",
      "1            Atheist      12        27       37       52        35       70\n",
      "2           Buddhist      27        21       30       34        33       58\n",
      "3           Catholic     418       617      732      670       638     1116\n",
      "4  Dont know/refused      15        14       15       11        10       35\n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "                   religion   <$10k   $10-20k  $20-30k  $30-40k   $40-50k  \\\n",
      "5         Evangelical Prot      575       869     1064      982       881   \n",
      "6                    Hindu        1         9        7        9        11   \n",
      "7  Historically Black Prot      228       244      236      238       197   \n",
      "8         Jehovahs Witness       20        27       24       24        21   \n",
      "9                   Jewish       19        19       25       25        30   \n",
      "\n",
      "   $50-75k  \n",
      "5     1486  \n",
      "6       34  \n",
      "7      223  \n",
      "8       30  \n",
      "9       95  \n"
     ]
    }
   ],
   "source": [
    "print(df_pew.loc[0:10, ['religion', '$50-75k']])\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_pew.iloc[0:10])\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_pew.head(5))\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_pew.tail(5))   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What does not seem right in the above dataframe?***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Try to make the column headers represent a variable not a value. For that, use the `melt` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   religion  income  count\n",
      "0                  Agnostic   <$10k     27\n",
      "1                   Atheist   <$10k     12\n",
      "2                  Buddhist   <$10k     27\n",
      "3                  Catholic   <$10k    418\n",
      "4         Dont know/refused   <$10k     15\n",
      "5         Evangelical Prot    <$10k    575\n",
      "6                    Hindu    <$10k      1\n",
      "7  Historically Black Prot    <$10k    228\n",
      "8         Jehovahs Witness    <$10k     20\n",
      "9                   Jewish    <$10k     19\n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "                    religion   income  count\n",
      "50                  Agnostic  $50-75k    137\n",
      "51                   Atheist  $50-75k     70\n",
      "52                  Buddhist  $50-75k     58\n",
      "53                  Catholic  $50-75k   1116\n",
      "54         Dont know/refused  $50-75k     35\n",
      "55         Evangelical Prot   $50-75k   1486\n",
      "56                    Hindu   $50-75k     34\n",
      "57  Historically Black Prot   $50-75k    223\n",
      "58         Jehovahs Witness   $50-75k     30\n",
      "59                   Jewish   $50-75k     95\n"
     ]
    }
   ],
   "source": [
    "df_pew_tidy = pd.melt(df_pew, id_vars=['religion'], var_name='income', value_name='count')\n",
    "print(df_pew_tidy.head(10))\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_pew_tidy.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Billboard Dataset**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset outlines data about the top hit songs on the Billboard list and the week from entrance that it was in the billboard with the ranking."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Read the dataset and store it in a pandas dataframe. Note that the usual utf-8 encoding does not work on this dataset. The reason behind this is that there might be characters that are not supported by `utf-8`.\n",
    "\n",
    "The suggestion is to use for this dataset `unicode_escape` encoding. (converts all non-ASCII characters into their \\uXXXX representations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billboard = pd.read_csv('./Data/billboard.csv', encoding='unicode_escape')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Observe the first few rows of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "5  2000                Janet                  Doesn't Really Matter  4:17   \n",
      "6  2000      Destiny's Child                            Say My Name  4:31   \n",
      "7  2000    Iglesias, Enrique                            Be With You  3:36   \n",
      "8  2000                Sisqo                             Incomplete  3:52   \n",
      "9  2000             Lonestar                                 Amazed  4:25   \n",
      "\n",
      "     genre date.entered date.peaked  x1st.week  x2nd.week  x3rd.week  ...  \\\n",
      "0     Rock   2000-09-23  2000-11-18         78       63.0       49.0  ...   \n",
      "1     Rock   2000-02-12  2000-04-08         15        8.0        6.0  ...   \n",
      "2     Rock   1999-10-23  2000-01-29         71       48.0       43.0  ...   \n",
      "3     Rock   2000-08-12  2000-09-16         41       23.0       18.0  ...   \n",
      "4     Rock   2000-08-05  2000-10-14         57       47.0       45.0  ...   \n",
      "5     Rock   2000-06-17  2000-08-26         59       52.0       43.0  ...   \n",
      "6     Rock   1999-12-25  2000-03-18         83       83.0       44.0  ...   \n",
      "7    Latin   2000-04-01  2000-06-24         63       45.0       34.0  ...   \n",
      "8     Rock   2000-06-24  2000-08-12         77       66.0       61.0  ...   \n",
      "9  Country   1999-06-05  2000-03-04         81       54.0       44.0  ...   \n",
      "\n",
      "   x67th.week  x68th.week  x69th.week  x70th.week  x71st.week  x72nd.week  \\\n",
      "0         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "1         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "2         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "3         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "4         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "5         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "6         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "7         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "8         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "9         NaN         NaN         NaN         NaN         NaN         NaN   \n",
      "\n",
      "   x73rd.week  x74th.week  x75th.week  x76th.week  \n",
      "0         NaN         NaN         NaN         NaN  \n",
      "1         NaN         NaN         NaN         NaN  \n",
      "2         NaN         NaN         NaN         NaN  \n",
      "3         NaN         NaN         NaN         NaN  \n",
      "4         NaN         NaN         NaN         NaN  \n",
      "5         NaN         NaN         NaN         NaN  \n",
      "6         NaN         NaN         NaN         NaN  \n",
      "7         NaN         NaN         NaN         NaN  \n",
      "8         NaN         NaN         NaN         NaN  \n",
      "9         NaN         NaN         NaN         NaN  \n",
      "\n",
      "[10 rows x 83 columns]\n"
     ]
    }
   ],
   "source": [
    "print(df_billboard.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is wrong with the above dataset?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are plenty of things that are not right about the above dataset.\n",
    "\n",
    " One of the things is that the column naming is very inconsistent, meaning column names like x1st.week, x2nd.week are not standard and include a prefix 'x' which is unnecessary and confusing. \n",
    "\n",
    " Secondly, the representation of the dataset is really sparse, meaning a few values are present in many columns and the structure is very inefficient.\n",
    "\n",
    " Lastly, from around the x67th.week to x72nd.week (maybe even on earlier weeks, but that is all I can see from the output), all values are NaN and it is taking a lot of memory space for storing those values and it also making harder for the run-time to go through all the dataset."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Let's, again, use the `melt` function to fix the general structure of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "5  2000                Janet                  Doesn't Really Matter  4:17   \n",
      "6  2000      Destiny's Child                            Say My Name  4:31   \n",
      "7  2000    Iglesias, Enrique                            Be With You  3:36   \n",
      "8  2000                Sisqo                             Incomplete  3:52   \n",
      "9  2000             Lonestar                                 Amazed  4:25   \n",
      "\n",
      "     genre date.entered date.peaked       week  rank  \n",
      "0     Rock   2000-09-23  2000-11-18  x1st.week  78.0  \n",
      "1     Rock   2000-02-12  2000-04-08  x1st.week  15.0  \n",
      "2     Rock   1999-10-23  2000-01-29  x1st.week  71.0  \n",
      "3     Rock   2000-08-12  2000-09-16  x1st.week  41.0  \n",
      "4     Rock   2000-08-05  2000-10-14  x1st.week  57.0  \n",
      "5     Rock   2000-06-17  2000-08-26  x1st.week  59.0  \n",
      "6     Rock   1999-12-25  2000-03-18  x1st.week  83.0  \n",
      "7    Latin   2000-04-01  2000-06-24  x1st.week  63.0  \n",
      "8     Rock   2000-06-24  2000-08-12  x1st.week  77.0  \n",
      "9  Country   1999-06-05  2000-03-04  x1st.week  81.0  \n"
     ]
    }
   ],
   "source": [
    "df_billboard_melted = pd.melt(\n",
    "    df_billboard,\n",
    "    id_vars=['year', 'artist.inverted', 'track', 'time', 'genre', 'date.entered', 'date.peaked'],\n",
    "    var_name='week',\n",
    "    value_name='rank'\n",
    ")\n",
    "print(df_billboard_melted.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we inspect the current dataframe. We find that it is structured in a better way than before. \n",
    "\n",
    "However, the ***Week*** column looks a bit ugly!\n",
    "\n",
    "4. Let's try to place only the week number in that column without the extras surronding it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   year      artist.inverted                                  track  time  \\\n",
      "0  2000      Destiny's Child               Independent Women Part I  3:38   \n",
      "1  2000              Santana                           Maria, Maria  4:18   \n",
      "2  2000        Savage Garden                     I Knew I Loved You  4:07   \n",
      "3  2000              Madonna                                  Music  3:45   \n",
      "4  2000  Aguilera, Christina  Come On Over Baby (All I Want Is You)  3:38   \n",
      "5  2000                Janet                  Doesn't Really Matter  4:17   \n",
      "6  2000      Destiny's Child                            Say My Name  4:31   \n",
      "7  2000    Iglesias, Enrique                            Be With You  3:36   \n",
      "8  2000                Sisqo                             Incomplete  3:52   \n",
      "9  2000             Lonestar                                 Amazed  4:25   \n",
      "\n",
      "     genre date.entered date.peaked  week  rank  \n",
      "0     Rock   2000-09-23  2000-11-18   1.0  78.0  \n",
      "1     Rock   2000-02-12  2000-04-08   1.0  15.0  \n",
      "2     Rock   1999-10-23  2000-01-29   1.0  71.0  \n",
      "3     Rock   2000-08-12  2000-09-16   1.0  41.0  \n",
      "4     Rock   2000-08-05  2000-10-14   1.0  57.0  \n",
      "5     Rock   2000-06-17  2000-08-26   1.0  59.0  \n",
      "6     Rock   1999-12-25  2000-03-18   1.0  83.0  \n",
      "7    Latin   2000-04-01  2000-06-24   1.0  63.0  \n",
      "8     Rock   2000-06-24  2000-08-12   1.0  77.0  \n",
      "9  Country   1999-06-05  2000-03-04   1.0  81.0  \n"
     ]
    }
   ],
   "source": [
    "df_billboard_melted['week'] = df_billboard_melted['week'].str.extract(r'(\\d+)').astype(float)\n",
    "print(df_billboard_melted.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "5. Now let's inspect the ***Week*** column in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count    24092.000000\n",
      "mean        38.500000\n",
      "std         21.937866\n",
      "min          1.000000\n",
      "25%         19.750000\n",
      "50%         38.500000\n",
      "75%         57.250000\n",
      "max         76.000000\n",
      "Name: week, dtype: float64\n",
      "[ 1.  2.  3.  4.  5.  6.  7.  8.  9. 10. 11. 12. 13. 14. 15. 16. 17. 18.\n",
      " 19. 20. 21. 22. 23. 24. 25. 26. 27. 28. 29. 30. 31. 32. 33. 34. 35. 36.\n",
      " 37. 38. 39. 40. 41. 42. 43. 44. 45. 46. 47. 48. 49. 50. 51. 52. 53. 54.\n",
      " 55. 56. 57. 58. 59. 60. 61. 62. 63. 64. 65. 66. 67. 68. 69. 70. 71. 72.\n",
      " 73. 74. 75. 76.]\n"
     ]
    }
   ],
   "source": [
    "print(df_billboard_melted['week'].describe())\n",
    "print(df_billboard_melted['week'].unique())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's try to find the date at which the song ranked the number that is shown per row.\n",
    "\n",
    "6. To do that let's first think of the equation that is going to get us the relevant date at which the song ranked the *rth*.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Equation: \n",
    "\n",
    "Ranked Date = Start Date + (Week Number − 1) × 7 days\n",
    "\n",
    "or in python:\n",
    "\n",
    "df['ranked_date'] = start_date + pd.to_timedelta((df['week'] - 1) * 7, unit='D')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billboard_melted['date.entered'] = pd.to_datetime(df_billboard_melted['date.entered'])\n",
    "\n",
    "df_billboard_melted['ranked_date'] = df_billboard_melted['date.entered'] + pd.to_timedelta((df_billboard_melted['week'] - 1) * 7, unit='D')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Timedeltas are absolute differences in times, expressed in difference units (e.g. days, hours, minutes, seconds). This method converts an argument from a recognized timedelta format / value into a Timedelta type.*\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***What is the problem with the calculation above?***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It will be off by one. The formula correctly uses (week - 1) to avoid assuming that week 1 happens 7 days after date.entered. If this were omitted, dates would be pushed one week too far.\n",
    "\n",
    "Data type mismatch: If week is not an integer or contains NaN, the timedelta operation could fail.\n",
    "\n",
    "Missing or malformed date.entered values: If any dates are not properly parsed to datetime, the addition will throw an error.\n",
    "\n",
    "So the main risk lies in data quality: missing, malformed, or NaN values in date.entered or week."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "7. Let's only keep necessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_billboard_melted = df_billboard_melted[['track', 'artist.inverted', 'week', 'rank', 'date.entered']]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "8. How to rename your columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33116\\3392744671.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_billboard_melted.rename(columns={\n"
     ]
    }
   ],
   "source": [
    "df_billboard_melted.rename(columns={\n",
    "    'track': 'Song',\n",
    "    'artist.inverted': 'Performer',\n",
    "    'week': 'WeekNumber',\n",
    "    'rank': 'Rank',\n",
    "    'date.entered': 'StartDate'\n",
    "}, inplace=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Display the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Song            Performer  WeekNumber  \\\n",
      "0               Independent Women Part I      Destiny's Child         1.0   \n",
      "1                           Maria, Maria              Santana         1.0   \n",
      "2                     I Knew I Loved You        Savage Garden         1.0   \n",
      "3                                  Music              Madonna         1.0   \n",
      "4  Come On Over Baby (All I Want Is You)  Aguilera, Christina         1.0   \n",
      "5                  Doesn't Really Matter                Janet         1.0   \n",
      "6                            Say My Name      Destiny's Child         1.0   \n",
      "7                            Be With You    Iglesias, Enrique         1.0   \n",
      "8                             Incomplete                Sisqo         1.0   \n",
      "9                                 Amazed             Lonestar         1.0   \n",
      "\n",
      "   Rank  StartDate  \n",
      "0  78.0 2000-09-23  \n",
      "1  15.0 2000-02-12  \n",
      "2  71.0 1999-10-23  \n",
      "3  41.0 2000-08-12  \n",
      "4  57.0 2000-08-05  \n",
      "5  59.0 2000-06-17  \n",
      "6  83.0 1999-12-25  \n",
      "7  63.0 2000-04-01  \n",
      "8  77.0 2000-06-24  \n",
      "9  81.0 1999-06-05  \n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "                   Song         Performer  WeekNumber  Rank  StartDate\n",
      "24082            Get Up    Larrieux, Amel        76.0   NaN 2000-03-04\n",
      "24083    Spanish Guitar     Braxton, Toni        76.0   NaN 2000-12-02\n",
      "24084            I Know           Tuesday        76.0   NaN 2000-12-30\n",
      "24085      Imagine That         LL Cool J        76.0   NaN 2000-08-12\n",
      "24086           Souljas          Master P        76.0   NaN 2000-11-18\n",
      "24087  Cherchez LaGhost  Ghostface Killah        76.0   NaN 2000-08-05\n",
      "24088       Freakin' It       Smith, Will        76.0   NaN 2000-02-12\n",
      "24089     Kernkraft 400     Zombie Nation        76.0   NaN 2000-09-02\n",
      "24090          Got Beef    Eastsidaz, The        76.0   NaN 2000-07-01\n",
      "24091    Toca's Miracle            Fragma        76.0   NaN 2000-10-28\n"
     ]
    }
   ],
   "source": [
    "print(df_billboard_melted.head(10))\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_billboard_melted.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above dataframe, there are some *NaN* values. What are we going to do? <br/>\n",
    "9. Apply quick data cleaning and then observe the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do not need to drop all the rows that contain one NaN value, because that is a record that we do not want to lose. What is more convenient is to keep all the rows but fill in the blanks with a default rank of 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33116\\1136958303.py:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_billboard_melted['Rank'] = df_billboard_melted['Rank'].fillna(0)\n"
     ]
    }
   ],
   "source": [
    "df_billboard_melted['Rank'] = df_billboard_melted['Rank'].fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                    Song            Performer  WeekNumber  \\\n",
      "0               Independent Women Part I      Destiny's Child         1.0   \n",
      "1                           Maria, Maria              Santana         1.0   \n",
      "2                     I Knew I Loved You        Savage Garden         1.0   \n",
      "3                                  Music              Madonna         1.0   \n",
      "4  Come On Over Baby (All I Want Is You)  Aguilera, Christina         1.0   \n",
      "5                  Doesn't Really Matter                Janet         1.0   \n",
      "6                            Say My Name      Destiny's Child         1.0   \n",
      "7                            Be With You    Iglesias, Enrique         1.0   \n",
      "8                             Incomplete                Sisqo         1.0   \n",
      "9                                 Amazed             Lonestar         1.0   \n",
      "\n",
      "   Rank  StartDate  \n",
      "0  78.0 2000-09-23  \n",
      "1  15.0 2000-02-12  \n",
      "2  71.0 1999-10-23  \n",
      "3  41.0 2000-08-12  \n",
      "4  57.0 2000-08-05  \n",
      "5  59.0 2000-06-17  \n",
      "6  83.0 1999-12-25  \n",
      "7  63.0 2000-04-01  \n",
      "8  77.0 2000-06-24  \n",
      "9  81.0 1999-06-05  \n",
      "--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\n",
      "                   Song         Performer  WeekNumber  Rank  StartDate\n",
      "24082            Get Up    Larrieux, Amel        76.0   0.0 2000-03-04\n",
      "24083    Spanish Guitar     Braxton, Toni        76.0   0.0 2000-12-02\n",
      "24084            I Know           Tuesday        76.0   0.0 2000-12-30\n",
      "24085      Imagine That         LL Cool J        76.0   0.0 2000-08-12\n",
      "24086           Souljas          Master P        76.0   0.0 2000-11-18\n",
      "24087  Cherchez LaGhost  Ghostface Killah        76.0   0.0 2000-08-05\n",
      "24088       Freakin' It       Smith, Will        76.0   0.0 2000-02-12\n",
      "24089     Kernkraft 400     Zombie Nation        76.0   0.0 2000-09-02\n",
      "24090          Got Beef    Eastsidaz, The        76.0   0.0 2000-07-01\n",
      "24091    Toca's Miracle            Fragma        76.0   0.0 2000-10-28\n"
     ]
    }
   ],
   "source": [
    "print(df_billboard_melted.head(10))\n",
    "print(\"--------------------*-------------------*-------------------*-------------------*-------------------*-------------------\")\n",
    "print(df_billboard_melted.tail(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Data Cleaning**\n",
    "\n",
    "Data cleaning involves removing unwanted characters, imputing, or dropping missing values.\n",
    "\n",
    "The decision is based on the dataset you have, and the information you can extract from the other columns.\n",
    "\n",
    "\n",
    "Examples of data cleaning include cleaning:\n",
    "\n",
    "1.   **Missing Data**\n",
    "2.   **Irregular Data** (Outliers)\n",
    "3.   **Unnecessary Data** — Repetitive Data, Duplicates and more\n",
    "4.   **Inconsistent Data** — Capitalization, Addresses and more\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Data Set**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start by reading the dataset related to car models: ./CSVs/cars.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars = pd.read_csv('./Data/cars.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First Row seems to be the datatype, we need to remove it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin\n",
      "0  Chevrolet Chevelle Malibu;;8;307.0;130.0;3504....                        \n",
      "1   Buick Skylark 320;15.0;8;350.0;;3693.;11.5;70;US                        \n",
      "2  Plymouth Satellite;;8;318.0;150.0;3436.;11.0;7...                        \n",
      "3            AMC Rebel SST;16.0;8;;150.0;;12.0;70;US                        \n",
      "4    Ford Torino;17.0;8;302.0;140.0;3449.;10.5;70;US                        \n",
      "5        Ford Galaxie 500;;8;429.0;;4341.;10.0;70;US                        \n",
      "6  Chevrolet Impala;14.0;8;454.0;220.0;4354.;9.0;...                        \n",
      "7  Plymouth Fury iii;14.0;8;440.0;215.0;4312.;8.5...                        \n",
      "8  Pontiac Catalina;14.0;8;455.0;225.0;4425.;10.0...                        \n",
      "9  AMC Ambassador DPL;15.0;8;390.0;190.0;3850.;8....                        \n"
     ]
    }
   ],
   "source": [
    "df_cars = df_cars.iloc[1:].reset_index(drop=True)\n",
    "print(df_cars.head(10))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe the columns with null values.  Either by using the `isnull().sum()` function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\", df_cars.isnull().sum(), \"\\n\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There aren't many missing values. Let's take a glimpse at the percentage of the missing values:\n",
    "\n",
    "**HINT:** We'll need `Numpy` for the below task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.00%\n"
     ]
    }
   ],
   "source": [
    "total_cells = np.prod(df_cars.shape)\n",
    "\n",
    "missing_values = np.count_nonzero(df_cars.isnull().to_numpy())\n",
    "\n",
    "missing_percentage = (missing_values / total_cells) * 100\n",
    "\n",
    "print(f\"Missing values: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More detailed checking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Count of cells with 'NaN' as string:\n",
      "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64\n",
      "\n",
      "Count of cells with 'n/a' as string:\n",
      "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64\n",
      "\n",
      "Count of cells with 'NULL' as string:\n",
      "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64\n",
      "Count of empty string ('') values:\n",
      "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64\n",
      "\n",
      "Count of whitespace-only string values:\n",
      "Car;MPG;Cylinders;Displacement;Horsepower;Weight;Acceleration;Model;Origin    0\n",
      "dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\AppData\\Local\\Temp\\ipykernel_33116\\669152982.py:14: FutureWarning: DataFrame.applymap has been deprecated. Use DataFrame.map instead.\n",
      "  print(df_cars.applymap(lambda x: isinstance(x, str) and x.isspace()).sum())\n"
     ]
    }
   ],
   "source": [
    "print(\"Count of cells with 'NaN' as string:\")\n",
    "print((df_cars == 'NaN').sum())\n",
    "\n",
    "print(\"\\nCount of cells with 'n/a' as string:\")\n",
    "print((df_cars == 'n/a').sum())\n",
    "\n",
    "print(\"\\nCount of cells with 'NULL' as string:\")\n",
    "print((df_cars == 'NULL').sum())\n",
    "\n",
    "print(\"Count of empty string ('') values:\")\n",
    "print((df_cars == '').sum())\n",
    "\n",
    "print(\"\\nCount of whitespace-only string values:\")\n",
    "print(df_cars.applymap(lambda x: isinstance(x, str) and x.isspace()).sum())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around *0.19%* of the values are missing, which isn't a lot. Therefore, we might go with the option of dropping all the rows with null values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dropping missing values: 406\n",
      "Rows after dropping missing values: 406\n"
     ]
    }
   ],
   "source": [
    "rows_before = df_cars.shape[0]\n",
    "print(\"Rows before dropping missing values:\", rows_before)\n",
    "\n",
    "df_cars = df_cars.dropna()\n",
    "\n",
    "rows_after = df_cars.shape[0]\n",
    "print(\"Rows after dropping missing values:\", rows_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I checked for missing values in rows in every single way.\n",
    "\n",
    " Tried the following:\n",
    "\n",
    "1. Using the .isnull().sum() method\n",
    "\n",
    "2. Checking for the string \"NaN\"\n",
    "\n",
    "3. Checking for the string \"n/a\"\n",
    "\n",
    "4. Checking for the string \"NULL\"\n",
    "\n",
    "5. Checking for the missing value manually or string \"\"\n",
    "\n",
    "6. Checing for the whitespaces-only\n",
    "\n",
    "All of them gave me a 0% missing values in rows."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets also check dropping the columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_cars_cleaned = df_billboard.dropna(axis=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's observe how many columns we lost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original number of columns: 1\n",
      "Number of columns after dropping those with missing values: 1\n",
      "Number of columns dropped: 0\n"
     ]
    }
   ],
   "source": [
    "original_cols = df_cars.shape[1]\n",
    "cleaned_cols = df_cars.shape[1]\n",
    "dropped_cols = original_cols - cleaned_cols\n",
    "\n",
    "print(\"Original number of columns:\", original_cols)\n",
    "print(\"Number of columns after dropping those with missing values:\", cleaned_cols)\n",
    "print(\"Number of columns dropped:\", dropped_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The problem is that we only have one column and all the data is stored as a string. \n",
    "We need to seperate all this one column into many seperate/distinct columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                         Car   MPG Cylinders Displacement Horsepower Weight  \\\n",
      "0  Chevrolet Chevelle Malibu               8        307.0      130.0  3504.   \n",
      "1          Buick Skylark 320  15.0         8        350.0             3693.   \n",
      "2         Plymouth Satellite               8        318.0      150.0  3436.   \n",
      "3              AMC Rebel SST  16.0         8                   150.0          \n",
      "4                Ford Torino  17.0         8        302.0      140.0  3449.   \n",
      "\n",
      "  Acceleration Model Origin  \n",
      "0         12.0    70     US  \n",
      "1         11.5    70     US  \n",
      "2         11.0    70     US  \n",
      "3         12.0    70     US  \n",
      "4         10.5    70     US  \n"
     ]
    }
   ],
   "source": [
    "df_split = df_cars.iloc[:, 0].str.split(';', expand=True)\n",
    "\n",
    "df_split.columns = ['Car', 'MPG', 'Cylinders', 'Displacement', 'Horsepower', 'Weight', 'Acceleration', 'Model', 'Origin']\n",
    "\n",
    "print(df_split.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check for missing values again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values: 0.19%\n"
     ]
    }
   ],
   "source": [
    "df_split.replace('', np.nan, inplace=True)\n",
    "\n",
    "total_cells = np.prod(df_split.shape)\n",
    "\n",
    "missing_values = np.count_nonzero(df_split.isnull().to_numpy())\n",
    "\n",
    "missing_percentage = (missing_values / total_cells) * 100\n",
    "\n",
    "print(f\"Missing values: {missing_percentage:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rows before dropping missing values: 406\n",
      "Rows after dropping missing values: 401\n"
     ]
    }
   ],
   "source": [
    "rows_before = df_split.shape[0]\n",
    "print(\"Rows before dropping missing values:\", rows_before)\n",
    "\n",
    "df_split = df_split.dropna()\n",
    "\n",
    "rows_after = df_split.shape[0]\n",
    "print(\"Rows after dropping missing values:\", rows_after)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### All the rows with the missing values are finally dropped!!!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Cars Dataset - Filling in missing values automatically**\n",
    "\n",
    "Another option is to try and fill in the missing values through imputations."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's take the `MPG` column for example. We can fill in the missing values with 0s through the following line of code:\n",
    "\n",
    "`df_cars.fillna(0) `. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, this does not make much sense as there isn't MPG equal to 0. How about we plot the MPG column and if it follows a random distribution we can use the mean of the column to compute the missing values. Otherwise, we can use the median (if there is a skewed normal distribution). However, there might be a better way of imputation which is getting the median or the mean of the MPG of the cars with similar attributes."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we observe the graph above, we can consider it in a way or another normally distributed. Therefore, we can impute the missing values using the mean."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To compute the mean we need numeric values. However the values in the dataframe are objects. Therefore, we need to change them to numerics so that we can compute them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's see what is the mean of the MPG column"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use this mean to compute the missing values since the graph demonstarted a normal distribution"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Car Dataset - Simple Imputer**"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*SimpleImputer* is a `scikit-learn` class which is helpful in handling the missing data in the predictive model dataset. It replaces the `NaN` values with a specified placeholder.\n",
    "It is implemented by the use of the `SimpleImputer()` method which takes the following arguments :\n",
    "\n",
    "`missing_values` : The missing_values placeholder which has to be imputed. By default is NaN\n",
    "\n",
    "`strategy` : The data which will replace the NaN values from the dataset. The strategy argument can take the values – ‘mean'(default), ‘median’, ‘most_frequent’ and ‘constant’.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by importing the `SimpleImputer` into our notebook"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What we need to do are two essential steps:\n",
    "\n",
    "1. fit the data (compute the mean / median / most freq)\n",
    "2. transform the data (place the computed values in the NaN cells)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Outlier Detection** \n",
    "\n",
    "\n",
    "An Outlier is a data-item/object that deviates significantly from the rest of the (so-called normal)objects. They can be caused by measurement or execution errors. The analysis for outlier detection is referred to as outlier mining. There are many ways to detect the outliers, and the removal process is the data frame same as removing a data item from the panda’s data frame.\n",
    "\n",
    "\n",
    "\n",
    "https://www.geeksforgeeks.org/detect-and-remove-the-outliers-using-python/\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Outliers Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using Box Plot\n",
    "It captures the summary of the data effectively and efficiently with only a simple box and whiskers. Boxplot summarizes sample data using 25th, 50th, and 75th percentiles. One can just get insights(quartiles, median, and outliers) into the dataset by just looking at its boxplot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Visualizing Outliers Using ScatterPlot.\n",
    "\n",
    "It is used when you have paired numerical data and when your dependent variable has multiple values for each reading independent variable, or when trying to determine the relationship between the two variables. In the process of utilizing the scatter plot, one can also use it for outlier detection."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Z-Score:\n",
    "Z- Score is also called a standard score. This value/score helps to understand that how far is the data point from the mean. And after setting up a threshold value one can utilize z score values of data points to define the outliers.\n",
    "<br>\n",
    "Zscore = (data_point -mean) / std. deviation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now to define an outlier threshold value is chosen which is generally 3.0. As 99.7% of the data points lie between +/- 3 standard deviation (using Gaussian Distribution approach).\n",
    "\n",
    "Rows where Z value is greater than 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### IQR (Inter-Quartile Range)\n",
    "Inter Quartile Range approach to finding the outliers is the most commonly used and most trusted approach used in the research field. <Br>\n",
    "IQR = Quartile3 - Quartile1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To define the outlier base value is defined above and below dataset’s normal range namely Upper and Lower bounds, define the upper and the lower bound (1.5*IQR value is considered) :<br>\n",
    "upper = Q3 + 1.5 * IQR <br>\n",
    "lower = Q1 - 1.5 * IQR <br> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Removing the outliers:\n",
    "For removing the outlier, one must follow the same process of removing an entry from the dataset using its exact position in the dataset because in all the above methods of detecting the outliers end result is the list of all those data items that satisfy the outlier definition according to the method used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
